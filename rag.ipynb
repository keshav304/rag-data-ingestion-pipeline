{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "87ee5576",
      "metadata": {},
      "source": [
        "# Product documentation → vector embeddings → MongoDB Atlas\n",
        "\n",
        "This notebook loads the product documentation PDF, chunks it, generates embeddings with a **free local model** (sentence-transformers), and stores documents + embeddings in MongoDB Atlas for vector search. No OpenAI or other API key required for embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "89d33caa",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import hashlib\n",
        "import time\n",
        "from datetime import datetime, timezone\n",
        "from pathlib import Path\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from pymongo.mongo_client import MongoClient\n",
        "from pymongo.server_api import ServerApi\n",
        "from pymongo.operations import ReplaceOne\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "MONGO_URI = os.getenv(\"MONGO_URI\")\n",
        "PDF_PATH = Path(\"PRODUCT_DOCUMENTATION.pdf\")\n",
        "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # 384 dimensions\n",
        "DOC_DB, DOC_COLL = \"dhsync\", \"product_docs\"\n",
        "\n",
        "# Vector index: must match embedding dimensions\n",
        "VECTOR_INDEX_NAME = \"vector_index\"\n",
        "EMBEDDING_DIMENSIONS = 384\n",
        "\n",
        "assert MONGO_URI, \"Set MONGO_URI in .env\"\n",
        "assert PDF_PATH.exists(), f\"PDF not found: {PDF_PATH}\"\n",
        "\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    \"\"\"Clean PDF text: broken lines, hyphenation, hidden chars, extra whitespace.\"\"\"\n",
        "    if not text or not text.strip():\n",
        "        return \"\"\n",
        "    # Replace hyphen-newline with nothing (join hyphenated words)\n",
        "    t = re.sub(r\"-\\s*\\n\\s*\", \"\", text)\n",
        "    # Normalize whitespace (including unicode) to single space\n",
        "    t = re.sub(r\"[\\s\\u00a0]+\", \" \", t)\n",
        "    # Strip control characters\n",
        "    t = \"\".join(c for c in t if c.isprintable() or c in \"\\n\\t\")\n",
        "    return t.strip()\n",
        "\n",
        "\n",
        "def content_hash(*parts: str) -> str:\n",
        "    \"\"\"Stable id for dedup/upsert.\"\"\"\n",
        "    return hashlib.sha256(\"|\".join(parts).encode(\"utf-8\")).hexdigest()[:24]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f3d358cb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 22 page(s), split into 93 chunks.\n"
          ]
        }
      ],
      "source": [
        "# Load PDF and split into chunks (raw text; we normalize when embedding/storing)\n",
        "loader = PyPDFLoader(str(PDF_PATH))\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=600,\n",
        "    chunk_overlap=120,\n",
        "    length_function=len,\n",
        "    separators=[\"\\n## \", \"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
        ")\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "print(f\"Loaded {len(documents)} page(s), split into {len(chunks)} chunks.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b9f48cc2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading embedding model: sentence-transformers/all-MiniLM-L6-v2 (first run may download ~80MB)...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/var/folders/t4/3ls2qr3s3nnfrd1hfk0j8lcc0000gn/T/ipykernel_53649/147371557.py:21: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
            "/Users/keshavjha/Desktop/vemb/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 753.71it/s, Materializing param=pooler.dense.weight]                             \n",
            "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "\u001b[3mNotes:\n",
            "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upserted 93 chunks (inserted: 93, updated: 0).\n"
          ]
        }
      ],
      "source": [
        "# Generate embeddings and upsert into MongoDB (no duplicates; safe to re-run)\n",
        "def embed_with_retry(embeddings, texts, batch_size=32, max_retries=3):\n",
        "    \"\"\"Embed in batches with retries for transient failures.\"\"\"\n",
        "    all_vectors = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i : i + batch_size]\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                all_vectors.extend(embeddings.embed_documents(batch))\n",
        "                break\n",
        "            except Exception as e:\n",
        "                if attempt == max_retries - 1:\n",
        "                    raise\n",
        "                time.sleep(2 ** attempt)\n",
        "    return all_vectors\n",
        "\n",
        "# Define doc_id once per document (required for this cell; does not depend on cell 2)\n",
        "doc_id = content_hash(str(PDF_PATH.resolve()))\n",
        "\n",
        "print(f\"Loading embedding model: {EMBEDDING_MODEL} (first run may download ~80MB)...\")\n",
        "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
        "\n",
        "# Embed normalized text for better quality; store cleaned + optional raw\n",
        "texts = [normalize_text(chunk.page_content) for chunk in chunks]\n",
        "try:\n",
        "    all_vectors = embed_with_retry(embeddings, texts)\n",
        "except Exception as e:\n",
        "    print(f\"Embedding failed: {e}\")\n",
        "    raise\n",
        "\n",
        "client = MongoClient(MONGO_URI, server_api=ServerApi(\"1\"), serverSelectionTimeoutMS=10000)\n",
        "db = client[DOC_DB]\n",
        "collection = db[DOC_COLL]\n",
        "\n",
        "now = datetime.now(timezone.utc)\n",
        "operations = []\n",
        "for position, (chunk, vec) in enumerate(zip(chunks, all_vectors)):\n",
        "    source = chunk.metadata.get(\"source\", \"\")\n",
        "    page = chunk.metadata.get(\"page\")\n",
        "    text_clean = texts[position]\n",
        "    chunk_id = content_hash(text_clean, source, str(page))\n",
        "    doc = {\n",
        "        \"_id\": chunk_id,\n",
        "        \"chunk_id\": chunk_id,\n",
        "        \"doc_id\": doc_id,\n",
        "        \"position\": position,\n",
        "        \"text\": text_clean,\n",
        "        \"text_raw\": chunk.page_content,\n",
        "        \"text_length\": len(text_clean),\n",
        "        \"embedding\": vec,\n",
        "        \"embedding_model\": EMBEDDING_MODEL,\n",
        "        \"created_at\": now,\n",
        "        \"metadata\": {\"source\": source, \"page\": page},\n",
        "    }\n",
        "    operations.append(ReplaceOne({\"_id\": chunk_id}, doc, upsert=True))\n",
        "\n",
        "# Upsert with retry for network/Atlas errors\n",
        "for attempt in range(3):\n",
        "    try:\n",
        "        if operations:\n",
        "            result = collection.bulk_write(operations, ordered=False)\n",
        "            print(f\"Upserted {result.upserted_count + result.modified_count} chunks (inserted: {result.upserted_count}, updated: {result.modified_count}).\")\n",
        "        else:\n",
        "            print(\"No chunks to write.\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        if attempt == 2:\n",
        "            print(f\"Write failed after retries: {e}\")\n",
        "            raise\n",
        "        time.sleep(2 ** attempt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "c4e46b92",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector search index already exists; skipping create.\n"
          ]
        }
      ],
      "source": [
        "# Create the vector search index in Atlas (required for $vectorSearch)\n",
        "# Option A: Automatically via Atlas Admin API (set in .env: ATLAS_GROUP_ID, ATLAS_CLUSTER_NAME, ATLAS_PUBLIC_KEY, ATLAS_PRIVATE_KEY)\n",
        "# Option B: Manually in Atlas UI — see the markdown cell below\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "ATLAS_GROUP_ID = os.getenv(\"ATLAS_GROUP_ID\")\n",
        "ATLAS_CLUSTER_NAME = os.getenv(\"ATLAS_CLUSTER_NAME\", \"Cluster0\")\n",
        "ATLAS_PUBLIC_KEY = os.getenv(\"ATLAS_PUBLIC_KEY\")\n",
        "ATLAS_PRIVATE_KEY = os.getenv(\"ATLAS_PRIVATE_KEY\")\n",
        "\n",
        "# Vector Search index (for $vectorSearch stage): definition uses top-level \"fields\" array\n",
        "index_definition = {\n",
        "    \"fields\": [\n",
        "        {\n",
        "            \"type\": \"vector\",\n",
        "            \"path\": \"embedding\",\n",
        "            \"numDimensions\": EMBEDDING_DIMENSIONS,\n",
        "            \"similarity\": \"cosine\",\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "payload = {\n",
        "    \"name\": VECTOR_INDEX_NAME,\n",
        "    \"type\": \"vectorSearch\",\n",
        "    \"database\": DOC_DB,\n",
        "    \"collectionName\": DOC_COLL,\n",
        "    \"definition\": index_definition,\n",
        "}\n",
        "\n",
        "if ATLAS_GROUP_ID and ATLAS_PUBLIC_KEY and ATLAS_PRIVATE_KEY:\n",
        "    import requests\n",
        "    from requests.auth import HTTPDigestAuth\n",
        "\n",
        "    base_url = f\"https://cloud.mongodb.com/api/atlas/v2/groups/{ATLAS_GROUP_ID}/clusters/{ATLAS_CLUSTER_NAME}/search/indexes\"\n",
        "    auth = HTTPDigestAuth(ATLAS_PUBLIC_KEY, ATLAS_PRIVATE_KEY)\n",
        "    headers = {\"Content-Type\": \"application/json\", \"Accept\": \"application/vnd.atlas.2024-05-30+json\"}\n",
        "    try:\n",
        "        # Idempotent: list existing indexes and skip if already present\n",
        "        list_r = requests.get(base_url, auth=auth, headers=headers, timeout=30)\n",
        "        data = list_r.json() if list_r.ok else None\n",
        "        index_list = data if isinstance(data, list) else (data.get(\"indexes\", []) if isinstance(data, dict) else [])\n",
        "        existing = {idx.get(\"name\") for idx in index_list if isinstance(idx, dict)}\n",
        "        if VECTOR_INDEX_NAME in existing:\n",
        "            print(\"An index named 'vector_index' already exists. If it is type 'Search' (not Vector Search), delete it in Atlas and re-run this cell to create a Vector Search index.\")\n",
        "        else:\n",
        "            r = requests.post(base_url, auth=auth, headers=headers, json=payload, timeout=30)\n",
        "            if r.status_code in (200, 201):\n",
        "                print(\"Vector Search index created. Status:\", r.json().get(\"status\", \"OK\"))\n",
        "            else:\n",
        "                print(\"Index creation returned:\", r.status_code, r.text[:500])\n",
        "                if r.status_code == 400:\n",
        "                    print(\"Create the Vector Search index manually in Atlas UI (see markdown cell below).\")\n",
        "    except Exception as e:\n",
        "        print(\"Atlas API error:\", e)\n",
        "else:\n",
        "    print(\"Set ATLAS_GROUP_ID, ATLAS_PUBLIC_KEY, ATLAS_PRIVATE_KEY (and optionally ATLAS_CLUSTER_NAME) in .env to create the index automatically.\")\n",
        "    print(\"Otherwise create it in the Atlas UI using the definition below (next cell).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3b95e74",
      "metadata": {},
      "source": [
        "## Vector Search index (manual fallback)\n",
        "\n",
        "**You must create a Vector Search index** (not a plain Atlas Search index) so `$vectorSearch` works.\n",
        "\n",
        "1. **Atlas** → **Database** → your cluster → **Search** tab.\n",
        "2. If an index named `vector_index` already exists and its **Type** is \"Search\", delete it (it is the wrong type).\n",
        "3. **Create Search Index** → **JSON Editor**.\n",
        "4. Database: `dhsync`, collection: `product_docs`, index name: `vector_index`.\n",
        "5. **Index type:** ensure you are creating a **Vector Search** index. Use this definition (top-level `fields` array, path `embedding`):\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"fields\": [\n",
        "    {\n",
        "      \"type\": \"vector\",\n",
        "      \"path\": \"embedding\",\n",
        "      \"numDimensions\": 384,\n",
        "      \"similarity\": \"cosine\"\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "6. Wait until status is **READY**. Then run the test query cell.\n",
        "\n",
        "**If your index failed with \"numDimensions is required\":** You used `dimensions` but Atlas expects **`numDimensions`**. Edit the index in Atlas and set the embedding field to `\"numDimensions\": 384` (not `\"dimensions\"`). If using mappings format, use:\n",
        "```json\n",
        "\"embedding\": { \"type\": \"vector\", \"numDimensions\": 384, \"similarity\": \"cosine\" }\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "322bec55",
      "metadata": {},
      "source": [
        "## Short diagnosis (run before test query)\n",
        "\n",
        "Run the cell below to verify: documents have `embedding` (384 dims), connection target, and that you need a **Vector Search** index (not Atlas Search) for `$vectorSearch`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "4591a3eb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Connection target: dhsync . product_docs\n",
            "2. Document count: 93\n",
            "3. Sample doc has 'embedding' with 384 dimensions (expected 384)\n",
            "4. For $vectorSearch you need a Vector Search index (type: Vector Search in Atlas),\n",
            "   with definition: fields array, path 'embedding', numDimensions 384\n"
          ]
        }
      ],
      "source": [
        "# Short diagnosis: verify data and connection (run Cell 1 first)\n",
        "from pymongo.mongo_client import MongoClient\n",
        "from pymongo.server_api import ServerApi\n",
        "\n",
        "client = MongoClient(MONGO_URI, server_api=ServerApi(\"1\"))\n",
        "db = client[DOC_DB]\n",
        "collection = db[DOC_COLL]\n",
        "\n",
        "print(\"1. Connection target:\", DOC_DB, \".\", DOC_COLL)\n",
        "count = collection.count_documents({})\n",
        "print(\"2. Document count:\", count)\n",
        "\n",
        "doc = collection.find_one({\"embedding\": {\"$exists\": True}})\n",
        "if doc:\n",
        "    dims = len(doc[\"embedding\"])\n",
        "    print(\"3. Sample doc has 'embedding' with\", dims, \"dimensions (expected 384)\")\n",
        "    if dims != EMBEDDING_DIMENSIONS:\n",
        "        print(\"   WARNING: dimension mismatch — index and query must use\", dims)\n",
        "else:\n",
        "    print(\"3. No document with 'embedding' found — run ingest cells first.\")\n",
        "\n",
        "print(\"4. For $vectorSearch you need a Vector Search index (type: Vector Search in Atlas),\")\n",
        "print(\"   with definition: fields array, path 'embedding', numDimensions\", EMBEDDING_DIMENSIONS)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e53f49bd",
      "metadata": {},
      "source": [
        "## Test: find relevant chunks for a question\n",
        "\n",
        "Run the cell below to query the vector index with a test question and see the top matching chunks from your product docs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "3901d315",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collection dhsync.product_docs has 93 documents.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 1154.49it/s, Materializing param=pooler.dense.weight]                             \n",
            "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "\u001b[3mNotes:\n",
            "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Question: how to set office status\n",
            "\n",
            "Top 5 relevant chunks:\n",
            "\n",
            "--- Result 1 (score: 0.6988, page: 1) ---\n",
            "windows to their entries. • Use bulk operations to set status across multiple dates at once. • Create, manage, and apply personal templates for quick status entry. • Use advanced features such as Repeat Pattern, Copy From Date, and Copy Week/Month. • View the Today’s Status widget showing who is in the oﬀice, on leave, or working from home today. • Search and filter the team calendar by name, email, or status. • Change their own display name and password through the Profile page. Members are res...\n",
            "\n",
            "--- Result 2 (score: 0.6864, page: 19) ---\n",
            "a duplicate, due to the upsert behavior used by the system. Status V alues The only valid status values that can be stored in the database are “oﬀice” and “leave. ” Attempting to set any other value through the API results in a validation error. The “clear” status used in bulk operations is a client-side concept that translates to deleting the entry (reverting to WFH). Time Window V alidation If a time window is provided, both the start time and end time must be specified together. Providing onl...\n",
            "\n",
            "--- Result 3 (score: 0.6718, page: 17) ---\n",
            "Setting Status for Multiple Days at Once The user selects multiple dates on My Calendar using drag selection or Ctrl+Click. Once dates are selected, the Bulk Action Toolbar appears. The user chooses a status (Oﬀice, Leave, or Clear), optionally adds a time window and note, and clicks “Apply. ” The status is applied to all selected dates in one operation. Using Repeat Pattern The user clicks the “Repeat Pattern” button in the toolbar. A modal appears where they select a status, choose one or more...\n",
            "\n",
            "--- Result 4 (score: 0.6707, page: 0) ---\n",
            "particular status (Oﬀice or Leave) on a specific date. Entries are the only persisted attendance records. • W ork F rom Home (WFH): WFH is the default, implicit status for any working day where no entry exists. It is never stored in the database; it is derived at query time. • W orking Days: Any calendar day that is not a weekend (Saturday or Sunday) and not a designated organizational holiday is considered a working day. • Holidays: Organization-wide non-working days configured by administrator...\n",
            "\n",
            "--- Result 5 (score: 0.6625, page: 1) ---\n",
            "2. User Roles and Permissions Roles dhSync defines exactly two roles: Member and Admin. Member Role A member is a standard user of the system. Members have the following capabilities: • View the Team Calendar showing all active team members’ statuses for any month. • View and edit their own personal calendar (My Calendar). • Set their own daily status to Oﬀice, Leave, or WFH for dates within the allowed editing window. • Add optional notes (up to 500 characters) and optional active hour time win...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test query: embed a question and find the most relevant chunks (run Cell 1 first)\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from pymongo.mongo_client import MongoClient\n",
        "from pymongo.server_api import ServerApi\n",
        "\n",
        "test_question = \"how to set office status\"  # change this to any question\n",
        "\n",
        "client = MongoClient(MONGO_URI, server_api=ServerApi(\"1\"))\n",
        "db = client[DOC_DB]\n",
        "collection = db[DOC_COLL]\n",
        "\n",
        "# Sanity check: ensure we have docs and the index can be used\n",
        "count = collection.count_documents({})\n",
        "print(f\"Collection {DOC_DB}.{DOC_COLL} has {count} documents.\")\n",
        "if count == 0:\n",
        "    print(\"Run the ingest cells (load PDF + embed + upsert) first, then ensure the vector index is READY in Atlas.\")\n",
        "else:\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
        "    query_vector = embeddings.embed_query(test_question)\n",
        "\n",
        "    # Use $search with vectorSearch operator (Atlas Search index with mappings), not $vectorSearch stage\n",
        "    pipeline = [\n",
        "    {\n",
        "        \"$vectorSearch\": {\n",
        "            \"index\": VECTOR_INDEX_NAME,\n",
        "            \"path\": \"embedding\",\n",
        "            \"queryVector\": query_vector,\n",
        "            \"numCandidates\": 50,\n",
        "            \"limit\": 5,\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"$project\": {\n",
        "            \"text\": 1,\n",
        "            \"metadata\": 1,\n",
        "            \"score\": {\"$meta\": \"vectorSearchScore\"},\n",
        "        }\n",
        "    },\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        results = list(collection.aggregate(pipeline))\n",
        "    except Exception as e:\n",
        "        print(f\"Vector search failed: {e}\")\n",
        "        print(\"Check in Atlas: index exists, name 'vector_index', status READY, path 'embedding'.\")\n",
        "        results = []\n",
        "\n",
        "    print(f\"\\nQuestion: {test_question}\\n\")\n",
        "    print(f\"Top {len(results)} relevant chunks:\\n\")\n",
        "    if not results:\n",
        "        print(\"No results. If the collection has documents, check: index name, index status READY, and path 'embedding'.\")\n",
        "    for i, doc in enumerate(results, 1):\n",
        "        score = doc.get(\"score\")\n",
        "        meta = doc.get(\"metadata\") or {}\n",
        "        text = doc.get(\"text\") or \"\"\n",
        "        print(f\"--- Result {i} (score: {score:.4f}, page: {meta.get('page', '?')}) ---\")\n",
        "        print(text[:500] + (\"...\" if len(text) > 500 else \"\"))\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "07c6dea1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collection dhsync.product_docs has 93 documents.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 1659.87it/s, Materializing param=pooler.dense.weight]                             \n",
            "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "\u001b[3mNotes:\n",
            "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Question: how to set office status\n",
            "\n",
            "Top 5 relevant chunks:\n",
            "\n",
            "--- Result 1 (score: 0.6988, page: 1) ---\n",
            "windows to their entries. • Use bulk operations to set status across multiple dates at once. • Create, manage, and apply personal templates for quick status entry. • Use advanced features such as Repeat Pattern, Copy From Date, and Copy Week/Month. • View the Today’s Status widget showing who is in the oﬀice, on leave, or working from home today. • Search and filter the team calendar by name, email, or status. • Change their own display name and password through the Profile page. Members are res...\n",
            "\n",
            "--- Result 2 (score: 0.6864, page: 19) ---\n",
            "a duplicate, due to the upsert behavior used by the system. Status V alues The only valid status values that can be stored in the database are “oﬀice” and “leave. ” Attempting to set any other value through the API results in a validation error. The “clear” status used in bulk operations is a client-side concept that translates to deleting the entry (reverting to WFH). Time Window V alidation If a time window is provided, both the start time and end time must be specified together. Providing onl...\n",
            "\n",
            "--- Result 3 (score: 0.6718, page: 17) ---\n",
            "Setting Status for Multiple Days at Once The user selects multiple dates on My Calendar using drag selection or Ctrl+Click. Once dates are selected, the Bulk Action Toolbar appears. The user chooses a status (Oﬀice, Leave, or Clear), optionally adds a time window and note, and clicks “Apply. ” The status is applied to all selected dates in one operation. Using Repeat Pattern The user clicks the “Repeat Pattern” button in the toolbar. A modal appears where they select a status, choose one or more...\n",
            "\n",
            "--- Result 4 (score: 0.6707, page: 0) ---\n",
            "particular status (Oﬀice or Leave) on a specific date. Entries are the only persisted attendance records. • W ork F rom Home (WFH): WFH is the default, implicit status for any working day where no entry exists. It is never stored in the database; it is derived at query time. • W orking Days: Any calendar day that is not a weekend (Saturday or Sunday) and not a designated organizational holiday is considered a working day. • Holidays: Organization-wide non-working days configured by administrator...\n",
            "\n",
            "--- Result 5 (score: 0.6625, page: 1) ---\n",
            "2. User Roles and Permissions Roles dhSync defines exactly two roles: Member and Admin. Member Role A member is a standard user of the system. Members have the following capabilities: • View the Team Calendar showing all active team members’ statuses for any month. • View and edit their own personal calendar (My Calendar). • Set their own daily status to Oﬀice, Leave, or WFH for dates within the allowed editing window. • Add optional notes (up to 500 characters) and optional active hour time win...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test query: embed a question and find the most relevant chunks (run Cell 1 first)\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from pymongo.mongo_client import MongoClient\n",
        "from pymongo.server_api import ServerApi\n",
        "\n",
        "test_question = \"how to set office status\"  # change this to any question\n",
        "\n",
        "client = MongoClient(MONGO_URI, server_api=ServerApi(\"1\"))\n",
        "db = client[DOC_DB]\n",
        "collection = db[DOC_COLL]\n",
        "\n",
        "# Sanity check: ensure we have docs and the index can be used\n",
        "count = collection.count_documents({})\n",
        "print(f\"Collection {DOC_DB}.{DOC_COLL} has {count} documents.\")\n",
        "if count == 0:\n",
        "    print(\"Run the ingest cells (load PDF + embed + upsert) first, then ensure the vector index is READY in Atlas.\")\n",
        "else:\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
        "    query_vector = embeddings.embed_query(test_question)\n",
        "\n",
        "    # Use $search with vectorSearch operator (Atlas Search index with mappings), not $vectorSearch stage\n",
        "    pipeline = [\n",
        "    {\n",
        "        \"$vectorSearch\": {\n",
        "            \"index\": VECTOR_INDEX_NAME,\n",
        "            \"path\": \"embedding\",\n",
        "            \"queryVector\": query_vector,\n",
        "            \"numCandidates\": 50,\n",
        "            \"limit\": 5,\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"$project\": {\n",
        "            \"text\": 1,\n",
        "            \"metadata\": 1,\n",
        "            \"score\": {\"$meta\": \"vectorSearchScore\"},\n",
        "        }\n",
        "    },\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        results = list(collection.aggregate(pipeline))\n",
        "    except Exception as e:\n",
        "        print(f\"Vector search failed: {e}\")\n",
        "        print(\"Check in Atlas: index exists, name 'vector_index', status READY, path 'embedding'.\")\n",
        "        results = []\n",
        "\n",
        "    print(f\"\\nQuestion: {test_question}\\n\")\n",
        "    print(f\"Top {len(results)} relevant chunks:\\n\")\n",
        "    if not results:\n",
        "        print(\"No results. If the collection has documents, check: index name, index status READY, and path 'embedding'.\")\n",
        "    for i, doc in enumerate(results, 1):\n",
        "        score = doc.get(\"score\")\n",
        "        meta = doc.get(\"metadata\") or {}\n",
        "        text = doc.get(\"text\") or \"\"\n",
        "        print(f\"--- Result {i} (score: {score:.4f}, page: {meta.get('page', '?')}) ---\")\n",
        "        print(text[:500] + (\"...\" if len(text) > 500 else \"\"))\n",
        "        print()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
